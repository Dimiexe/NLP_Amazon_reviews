{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Import necessary modules and set constant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import nltk\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from joblib import dump\n",
    "import seaborn as sns\n",
    "nltk.download('wordnet')\n",
    "\n",
    "project_path = os.path.join(\"..\")\n",
    "json_file = os.path.join(project_path,\"Apps_for_Android_5.json\")\n",
    "SAVE_MODELS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Exploratory analysis\n",
    "## Load the json and explore the form of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(json_file, lines = True)\n",
    "print(\"There are \"+ str(df.shape[0]) + \" observations with \" + str(df.shape[1]) + \" columns each.\")\n",
    "print(df.head(10))\n",
    "print(\"Total NaN values: \" + str(df.isna().sum().sum()))\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get only relevant columns: \"reviewText\", \"overall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.loc[:,[\"reviewText\",\"overall\"]]\n",
    "#Since there are no NaN values in either of the columns we don't need to drop lines.\n",
    "# We will exclude from our process all 0-length reviews.\n",
    "data = data.loc[data.loc[:,\"reviewText\"].apply(lambda x: len(x)) > 0,:]\n",
    "data.reset_index(inplace = True, drop = True)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = plt.bar(data.overall.value_counts().sort_index().index,data.overall.value_counts().sort_index()/1000)\n",
    "plt.title(\"Distribution of review scores\")\n",
    "plt.ylabel(\"Reviews (thousands)\")\n",
    " \n",
    "i = 0\n",
    "for p in graph:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "     \n",
    "    plt.text(x+width/2,\n",
    "             y+height*1.01,\n",
    "             str((round(data.overall.value_counts(normalize = True).sort_index() * 10000)/100)[i+1])+'%',\n",
    "             ha='center',\n",
    "             weight='bold')\n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So we see that the classes are quite imbalanced, with \"overall\" = 5 dominating, with over half of the dataset's observations.\n",
    "#What about reviewText lengths? (That last one is for Word2vec approach, not followed after all.)\n",
    "lengths = []\n",
    "data.reviewText.apply(lambda x: lengths.append(len(x)))\n",
    "data.insert(1,column = \"lengths\",value = lengths)\n",
    "print(\"About review lengths\\n Mean: \"+str(data.loc[:,\"lengths\"].mean())+\"\\n Min: \"+str(data.loc[:,\"lengths\"].min())+\"\\n Max: \"+str(data.loc[:,\"lengths\"].max())+\"\\n Median: \"+str(data.loc[:,\"lengths\"].median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "reviews = []\n",
    "cleaned_sentences = []\n",
    "#Revoming e-mails\n",
    "data.reviewText = data.reviewText.apply(lambda x: re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)','', x))\n",
    "#Removing URLs\n",
    "data.reviewText = data.reviewText.apply(lambda x: re.sub(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '' , x))\n",
    "#Removing tags\n",
    "data.reviewText = data.reviewText.apply(lambda x: gensim.parsing.preprocessing.strip_tags(x))\n",
    "#Lowercasing and removing stop-words\n",
    "data.reviewText = data.reviewText.apply(lambda x: gensim.parsing.preprocessing.remove_stopwords(x.lower()))\n",
    "for rev in data.reviewText:\n",
    "    temp_review = []\n",
    "    for sent in rev.split('.'):\n",
    "        #If there is a sentence\n",
    "        if len(sent) > 0:\n",
    "            #Removing non alphanumeric chars\n",
    "            temp = gensim.parsing.preprocessing.strip_non_alphanum(sent)\n",
    "            #Removing numeric chars (keeping only letters)\n",
    "            temp = gensim.parsing.preprocessing.strip_numeric(temp)\n",
    "            #Removing words with 1 or 2 letters\n",
    "            temp = gensim.parsing.preprocessing.strip_short(temp, minsize = 3)\n",
    "            #Trimming possible multiple whitespaces\n",
    "            temp = gensim.parsing.preprocessing.strip_multiple_whitespaces(temp)\n",
    "            #Lemmatizing the sentences\n",
    "            lemma_sent = []\n",
    "            for word in temp.split():\n",
    "                lemma_sent.append(wnl.lemmatize(word))\n",
    "                temp_review.append(wnl.lemmatize(word))\n",
    "            cleaned_sentences.append(lemma_sent)\n",
    "    #Store the lematized sentences of a review as one string\n",
    "    reviews.append(temp_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Training Embeddings\n",
    "## Word2vec model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If there is time, I will try this method as well.\n",
    "#Uncoment to use.\n",
    "\"\"\"model_w2v = gensim.models.Word2Vec(sentences = cleaned_sentences, vector_size = 200, workers = 8, min_count = 5)\n",
    "vectors = np.asarray(model_w2v.wv.vectors)\n",
    "labels = np.asarray(model_w2v.wv.index_to_key)\n",
    "model_name = temporary_file(\"model_word2vec\")\n",
    "if SAVE_MODELS:\n",
    "    model_w2v.save(model_name)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncoment to train embeddings. If the trained model is available only run the last line to load it.\n",
    "\"\"\"def tagged_document(list_of_list_of_words):\n",
    "   for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "      yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "\n",
    "data_for_training = list(tagged_document(reviews))\n",
    "model_d2v = gensim.models.doc2vec.Doc2Vec(vector_size=200, min_count=5, epochs=30, workers = 8)\n",
    "model_d2v.build_vocab(data_for_training)\n",
    "model_d2v.train(data_for_training, total_examples=model_d2v.corpus_count, epochs=model_d2v.epochs)\n",
    "if SAVE_MODELS:\n",
    "   model_d2v.save(\"model_doc2vec_30epochs\")\"\"\"\n",
    "model_d2v = gensim.models.doc2vec.Doc2Vec.load(\"model_doc2vec_30epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Training Classifiers\n",
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([model_d2v.dv[x] for x in range(len(model_d2v.dv))])\n",
    "y = np.int8(np.array(data.overall))\n",
    "#Spliting Training and Testing\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(X, y, test_size=.15, shuffle=True, random_state = 42670, stratify = y) # random_state set for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with the imbalance of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to deal with imbalance by cutting \"overall\" == 5 observations in half.\n",
    "population_5 = round(data.loc[data.overall == 5,:].shape[0]/2)\n",
    "undersample = RandomUnderSampler(sampling_strategy={5: population_5})\n",
    "train_texts_under, train_labels_under = undersample.fit_resample(train_texts, train_labels)\n",
    "# Oversampling the classes for which I have less data\n",
    "population_1 = round(data.loc[data.overall == 1,:].shape[0] * 1.5)\n",
    "population_2 = round(data.loc[data.overall == 2,:].shape[0] * 2.2)\n",
    "population_3 = round(data.loc[data.overall == 3,:].shape[0] * 1.5)\n",
    "oversample = RandomOverSampler(sampling_strategy={1: population_1, 2: population_2, 3: population_3})\n",
    "train_texts_balanced, train_labels_balanced = oversample.fit_resample(train_texts_under, train_labels_under)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing balanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"overall\": train_labels_balanced})\n",
    "\n",
    "graph = plt.bar(df.overall.value_counts().sort_index().index,df.overall.value_counts().sort_index()/1000)\n",
    "plt.title(\"Balanced distributions\")\n",
    "plt.ylabel(\"Reviews (thousands)\")\n",
    " \n",
    "i = 0\n",
    "for p in graph:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "     \n",
    "    plt.text(x+width/2,\n",
    "             y+height*1.01,\n",
    "             str((round(df.overall.value_counts(normalize = True).sort_index() * 10000)/100)[i+1])+'%',\n",
    "             ha='center',\n",
    "             weight='bold')\n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definig and training the classifiers\n",
    "### Using 5-fold cross validation training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stratified because it helps with the imbalanced classes\n",
    "n_folds = 5\n",
    "kf = StratifiedKFold(n_splits = n_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_svm = []\n",
    "reports_svm = []\n",
    "count = 0\n",
    "for c_arg in [0.01, 0.1, 1, 10]:\n",
    "    print(\"C=\" +str(c_arg))\n",
    "    fold_count = 0\n",
    "    for train_index, test_index in kf.split(train_texts_balanced, train_labels_balanced):\n",
    "        print(\"kfold:\"+str(fold_count))\n",
    "        svm_model = LinearSVC(C=c_arg, class_weight=\"balanced\", random_state=42670)\n",
    "        X_train, X_valid = train_texts_balanced[train_index], train_texts_balanced[test_index]\n",
    "        y_train, y_valid = train_labels_balanced[train_index], train_labels_balanced[test_index]\n",
    "        svm_model.fit(X_train,y_train)\n",
    "        \n",
    "        val_labels_pred = svm_model.predict(X_valid)\n",
    "        \n",
    "        accuracies_svm.append(accuracy_score(val_labels_pred , y_valid))\n",
    "        reports_svm.append(classification_report(y_valid, val_labels_pred))\n",
    "        count += 1\n",
    "        fold_count += 1\n",
    "    avg_acc_score = sum(accuracies_svm[count-n_folds:count])/n_folds   \n",
    "    print(\"accuracy of each fold - {}\".format(accuracies_svm[count-n_folds:count]))\n",
    "    print(\"Avg accuracy : {}\".format(avg_acc_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the best model of c=0.01 on the whole of the training data.\n",
    "svm_model = LinearSVC(C=0.01, class_weight=\"balanced\", random_state=42670)\n",
    "svm_model.fit(train_texts_balanced,train_labels_balanced)\n",
    "if SAVE_MODELS:\n",
    "    dump(svm_model, 'svm_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_lr = []\n",
    "reports_lr = []\n",
    "count = 0\n",
    "for c_arg in [0.01, 0.1, 1, 10]:\n",
    "    print(\"C=\" +str(c_arg))\n",
    "    fold_count = 0\n",
    "    for train_index, test_index in kf.split(train_texts_balanced, train_labels_balanced):\n",
    "        print(\"kfold:\"+str(fold_count))\n",
    "        lr_model = LogisticRegression(C=c_arg, class_weight=\"balanced\", random_state=42670)\n",
    "        X_train, X_valid = train_texts_balanced[train_index], train_texts_balanced[test_index]\n",
    "        y_train, y_valid = train_labels_balanced[train_index], train_labels_balanced[test_index]\n",
    "        lr_model.fit(X_train,y_train)\n",
    "        \n",
    "        val_labels_pred = lr_model.predict(X_valid)\n",
    "        \n",
    "        accuracies_lr.append(accuracy_score(val_labels_pred , y_valid))\n",
    "        reports_lr.append(classification_report(y_valid, val_labels_pred))\n",
    "        count += 1\n",
    "        fold_count += 1\n",
    "    avg_acc_score = sum(accuracies_lr[count-n_folds:count])/n_folds   \n",
    "    print(\"accuracy of each fold - {}\".format(accuracies_lr[count-n_folds:count]))\n",
    "    print(\"Avg accuracy : {}\".format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the best model of c=0.01 on the whole of the training data.\n",
    "lr_model = LogisticRegression(C=0.01, class_weight=\"balanced\", random_state=42670)\n",
    "lr_model.fit(train_texts_balanced,train_labels_balanced)\n",
    "if SAVE_MODELS:\n",
    "    dump(lr_model, 'linReg_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_rf = []\n",
    "reports_rf = []\n",
    "count = 0\n",
    "for msl in [1, 2, 3, 4, 5]:\n",
    "    print(\"Min leaf=\"+str(msl))\n",
    "    fold_count = 0\n",
    "    for train_index, test_index in kf.split(train_texts_balanced, train_labels_balanced):\n",
    "        print(\"kfold:\"+str(fold_count))\n",
    "        rf_model = RFC(min_samples_leaf = msl, n_estimators = 1000, n_jobs = 7, random_state = 42670)\n",
    "        X_train, X_valid = train_texts_balanced[train_index], train_texts_balanced[test_index]\n",
    "        y_train, y_valid = train_labels_balanced[train_index], train_labels_balanced[test_index]\n",
    "        rf_model.fit(X_train,y_train)\n",
    "        \n",
    "        val_labels_pred = rf_model.predict(X_valid)\n",
    "        \n",
    "        accuracies_rf.append(accuracy_score(val_labels_pred , y_valid))\n",
    "        reports_rf.append(classification_report(y_valid, val_labels_pred))\n",
    "        count += 1\n",
    "        fold_count += 1\n",
    "    avg_acc_score = sum(accuracies_rf[count-n_folds:count])/n_folds\n",
    "    print(\"accuracy of each fold - {}\".format(accuracies_rf[count-n_folds:count]))\n",
    "    print('Avg accuracy : {}'.format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the best model of min_samples_leaf = 5 on the whole of the training data.\n",
    "rf_model = RFC(min_samples_leaf = 5, n_estimators = 1000, n_jobs = 8, random_state = 42670)\n",
    "rf_model.fit(train_texts_balanced,train_labels_balanced)\n",
    "if SAVE_MODELS:\n",
    "    dump(rf_model, 'RF_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.Test best models\n",
    "### From the previous training process and the reported metrics, the best performing models selected were:\n",
    "Linear SVM: c=0.01<br>\n",
    "Linear Regression: c=0.01<br>\n",
    "Random Forest: min_samples_leaf = 5<br><br>\n",
    "Below we are performing predictions on test data, evaluating and visualizing results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_preds = svm_model.predict(test_texts)\n",
    "report_svm = classification_report(test_labels, svm_preds)\n",
    "print(report_svm)\n",
    "conf_mat = confusion_matrix(test_labels, svm_preds)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=[1,2,3,4,5], yticklabels=[1,2,3,4,5])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds = lr_model.predict(test_texts)\n",
    "report_lr = classification_report(test_labels, lr_preds)\n",
    "print(report_lr)\n",
    "conf_mat = confusion_matrix(test_labels, lr_preds)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=[1,2,3,4,5], yticklabels=[1,2,3,4,5])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_preds = rf_model.predict(test_texts)\n",
    "report_rf = classification_report(test_labels, rf_preds)\n",
    "print(report_rf)\n",
    "conf_mat = confusion_matrix(test_labels, rf_preds)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=[1,2,3,4,5], yticklabels=[1,2,3,4,5])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "994a721acb18910134c59aaf4000377d75f663035e1597fe711af90cd4b53849"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv_deloitte': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
